# SpeechJudge-Data: A Large-Scale Human Feedback Corpus for Speech Generation

<div align="center">

<!-- Badges -->
[![arXiv](https://img.shields.io/badge/arXiv-2511.07931-b31b1b.svg)](https://arxiv.org/abs/2511.07931)
[![Demo Page](https://img.shields.io/badge/Project-Demo_Page-blue)](https://speechjudge.github.io/)
[![GitHub](https://img.shields.io/badge/GitHub-SpeechJudge-black?logo=github)](https://github.com/AmphionTeam/SpeechJudge)
[![Model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-yellow)](https://huggingface.co/RMSnow/SpeechJudge-GRM)
[![Data](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-yellow)](https://huggingface.co/datasets/RMSnow/SpeechJudge-Data)

</div>

## Introduction

**SpeechJudge-Data** is a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference.

This dataset accompanies the paper ***[SpeechJudge: Towards Human-Level Judgment for Speech Naturalness](https://arxiv.org/abs/2511.07931)*** and supports the training of the ***[SpeechJudge-GRM](https://huggingface.co/RMSnow/SpeechJudge-GRM)*** model.

## Dataset Structure

The dataset is organized into 4 splits. You can load specific splits based on your needs:

| Split | Description |
| :--- | :--- |
| **train** | Standard training set for reward model training. |
| **dev** | Validation set for hyperparameter tuning. |
| **test** | **SpeechJudge-Eval Benchmark**. This split contains only samples with **Full-Agreement (FA)** among different human raters, serving as a high-quality ground truth for benchmarking evaluation metrics. |
| **other** | Additional data (such as the `Tie` samples) not included in the primary splits. |

## Data Fields

Each row in the dataset contains the following fields. The structure is consistent across all splits.

### Meta Information
- **`index`** (`int64`): A unique identifier for the sample.
- **`subset`** (`string`): The source category of the prompt speech.
  - `regular`: Source from standard datasets (e.g., Emilia).
  - `expressive`: Source from expressive datasets (e.g., emotional, accented, whisper, or video game speech).
- **`language_setting`** (`string`): Indicates the language transfer task (Prompt Language $\to$ Target Language).
  - Values: `en2en`, `en2zh`, `zh2zh`, `zh2en`, `en2mixed`, `zh2mixed`.
  - Note: `mixed` refers to Chinese-English code-switching.
- **`chosen`** (`bool`): A quality filter flag.
  - `true`: The annotators for this sample have high agreement (>40%) with the global rater group. Recommended for training high-quality models.
  - `false`: Annotator agreement was lower (<=40%).

### Audio & Text Content
- **`prompt`** (`audio`): The reference prompt speech audio.
- **`prompt_text`** (`string`): The transcription of the prompt speech.
- **`target_text`** (`string`): The target text input for the TTS systems (Ground Truth text for Audio A and Audio B).
- **`audioA`** (`audio`): The first synthetic speech candidate.
- **`audioB`** (`audio`): The second synthetic speech candidate.

### Human Annotations & Labels
The following fields contain list-based annotations. The order of elements in these lists corresponds to the raters listed in the `rater` field.

- **`rater`** (`list` of `string`): The IDs of the human annotators (e.g., `["rater01", "rater05"]`).
- **`audioA_text_accuracy`** / **`audioB_text_accuracy`** (`list` of `int`): Binary intelligibility scores.
  - `0`: Contains intelligibility errors.
  - `1`: No errors.
- **`naturalness_annotation`** (`list` of `string`): Raw comparative ratings from each rater.
  - Values: `"A+1"`, `"A+2"`, `"B+1"`, `"B+2"`, `"Tie (missing reason)"`, `"Tie (both not good)"`, `"Tie (both very good)"`.
- **`naturalness_label`** (`string`): The aggregated ground truth label derived via **majority voting**.
  - Values: `A` (Audio A is better), `B` (Audio B is better), `Tie`.

### Model Output
- **`gemini-2.5-flash`** (`string`): The output generated by Gemini-2.5-Flash using Chain-of-Thought (CoT) prompting for this pair. (Note: May be empty for some samples).

## Usage

### 1. Loading the Dataset

You can load the dataset directly using the Hugging Face `datasets` library.

```python
from datasets import load_dataset

# Load the entire dataset (all splits)
ds = load_dataset("RMSnow/SpeechJudge-Data")

# Load a specific split, e.g., the SpeechJudge-Eval benchmark (test split)
test_ds = load_dataset("RMSnow/SpeechJudge-Data", split="test")
```

### 2. Filtering High-Quality Data (Experimental)

**Baseline Usage:**
The original models presented in our paper (**SpeechJudge-GRM** and **SpeechJudge-BTRM**) were trained on the full `train` split, which consists of approximately **42k samples**.

**Exploring Data Quality:**
To facilitate research into the impact of annotation quality, we include a `chosen` field. This field identifies samples annotated by raters who demonstrated a **high individual agreement rate (>40%)** with the global rater group, which consists of approximately **31k samples**.

We believe it is a valuable research direction to explore whether training exclusively on this high-consensus subset yields superior model performance compared to using the full dataset.

```python
# Filter the training set to keep only high-quality annotations
train_ds = ds['train']
high_quality_train = train_ds.filter(lambda x: x['chosen'] == True)

print(f"Original size: {len(train_ds)}")
print(f"Filtered size: {len(high_quality_train)}")
```

### 3. Accessing Annotation Details
Since annotation fields are lists corresponding to the `rater` list, you can access individual rater details as follows:

```python
sample = ds['train'][0]

# Print the naturalness rating given by the first rater
first_rater_name = sample['rater'][0]
first_rater_vote = sample['naturalness_annotation'][0]

print(f"Rater {first_rater_name} voted: {first_rater_vote}")
```

## Citation

If you use this dataset in your research, please cite our paper:

```bibtex
@article{zhang2025speechjudge,
  title={SpeechJudge: Towards Human-Level Judgment for Speech Naturalness},
  author={Zhang, Xueyao and Wang, Chaoren and Liao, Huan and Li, Ziniu and Wang, Yuancheng and Wang, Li and Jia, Dongya and Chen, Yuanzhe and Li, Xiulin and Chen, Zhuo and Wu, Zhizheng},
  journal={arXiv preprint arXiv:2511.07931},
  year={2025}
}
```

## License and Access

This dataset is gated. To access the data, please fill out the access request form with your name and organization. Access is granted automatically upon submission. By downloading this dataset, you agree to the terms of use described in the license.